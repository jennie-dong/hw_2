{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用DPO算法微调模型\n",
    "\n",
    "本教程演示如何使用DPO算法微调大模型（以Llama-3.1-8B模型为例）。通过本教程，你将学习如何配置训练参数，并使用 DPO 算法在具有偏好标签的数据上进行强化学习式的训练，从而提升模型在对齐任务中的性能。\n",
    "\n",
    "## 1. 什么是 DPO 算法？\n",
    "\n",
    "DPO（Direct Preference Optimization）是一种用于训练语言模型更好地对齐人类偏好的方法。它不依赖显式的奖励模型或策略梯度方法，而是直接在“人类偏好数据”上优化模型，使其在给定两个回答中更倾向于人类偏好的那个。\n",
    "\n",
    "## 2. 环境配置\n",
    "\n",
    "在开始之前，请确保您已安装 ``align-anything`` 包。\n",
    "\n",
    "```bash\n",
    "# 克隆仓库\n",
    "git clone git@github.com:PKU-Alignment/align-anything.git\n",
    "cd align-anything\n",
    "\n",
    "# 使用conda创建虚拟环境\n",
    "conda create -n align-anything python==3.11\n",
    "conda activate align-anything\n",
    "```\n",
    "\n",
    "- **`[Optional]`** We recommend installing [CUDA](https://anaconda.org/nvidia/cuda) in the conda environment and set the environment variable.\n",
    "\n",
    "```bash\n",
    "# 我们在 H800 计算集群上测试过，这个版本的 CUDA 效果很好。\n",
    "# 您可以根据计算集群的实际情况调整此版本。\n",
    "\n",
    "conda install nvidia/label/cuda-12.2.0::cuda\n",
    "export CUDA_HOME=$CONDA_PREFIX\n",
    "```\n",
    "\n",
    "> 如果您的 CUDA 安装在不同的位置，例如 `/usr/local/cuda/bin/nvcc`，您可以按如下方式设置环境变量：\n",
    "\n",
    "```bash\n",
    "export CUDA_HOME=\"/usr/local/cuda\"\n",
    "```\n",
    "\n",
    "最后，通过以下命令安装 `align-anything`：\n",
    "\n",
    "```bash\n",
    "# 我们为训练和评估准备了快速安装。\n",
    "# 如果您只需要使用训练或评估模块，\n",
    "# 您可以安装相应的依赖项。\n",
    "pip install -e .[train] # 安装训练依赖项\n",
    "pip install -e .[evaluate] # 安装评估依赖项\n",
    "\n",
    "# 如果您需要安装所有依赖项，可以使用以下命令：\n",
    "pip install -e .[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Qwen2.5-0.5B-Instruct模型输出示例\n",
    "下面，让我们首先测试Qwen2.5-0.5B-Instruct模型的zero-shot能力。\n",
    "### 3.1 导入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.52.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /data/phybench/miniconda3/envs/vis/lib/python3.13/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 加载原始的Qwen2.5-0.5B-Instruct 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"  \n",
    "model_path = \"/data/phybench/workdir/guosy/align/Qwen2.5-0.5B-Instruct\" \n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# 将模型设置为eval模式\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 测试原始模型的性能\n",
    "\n",
    "让我们用一个示例问题测试 Qwen2.5-0.5B-Instruct 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text: It's important to approach any wild animal with caution and respect. Here are some steps you can take:\n",
      "\n",
      "1. **Stay Calm**: Try to stay calm and composed so you don't accidentally provoke or attack the animal.\n",
      "\n",
      "2. **Do Not Feed or Touch**: Do not feed or touch the animal, as it may be frightened or aggressive due to fear of human contact.\n",
      "\n",
      "3. **Use Non-Weaponized Tools**: If you need to use tools like sticks, rocks, or other non-lethal items, do so carefully and slowly. Avoid using weapons unless absolutely necessary.\n",
      "\n",
      "4. **Call for Help**: If the situation is dangerous or if you feel threatened by the animal, call your local wildlife control agency or emergency services immediately. They will provide appropriate advice and assistance.\n",
      "\n",
      "5. **Secure Your Property**: Ensure that your property is secure enough to contain the animal safely. This might involve setting up barriers, placing stakes, or moving furniture away from the area where the animal was last seen.\n",
      "\n",
      "6. **Monitor the Area**: Continue monitoring the area around where the animal was last seen. Look for any signs of movement or unusual behavior that could indicate danger.\n",
      "\n",
      "7. **Follow Up**: After contacting authorities or waiting for them to arrive, follow their instructions closely about what they recommend doing next.\n",
      "\n",
      "Remember, handling wild animals responsibly is crucial to ensure their well-being and safety.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Recently, a wild animal in the local area has become aggressive towards humans and caused several injuries. How should I handle this wild animal?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# the model generate new tokens\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=2048)\n",
    "# convert the generated tokens to text\n",
    "generated_text = tokenizer.decode(\n",
    "    output[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True\n",
    ")\n",
    "print(\"\\nGenerated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用DPO算法对齐模型\n",
    "\n",
    "**注意**：如果您无法访问huggingface.co，请将huggingface的endpoint设置为hf-mirror.com。您可以进行以下操作：\n",
    "\n",
    "`export HF_ENDPOINT=\"https://hf-mirror.com\"`\n",
    "\n",
    "在这里，我们以 align-anything 数据集为例。\n",
    "\n",
    "可以参考如下的训练脚本：\n",
    "\n",
    "训练完成后，您可以在`OUTPUT_DIR`下找到训练的模型权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering valid indices: 100%|██████████| 1000/1000 [00:00<00:00, 3900.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: DPO Loss = 0.6914\n",
      "Step 10: DPO Loss = 0.6953\n",
      "Step 20: DPO Loss = 0.5000\n",
      "Step 30: DPO Loss = 0.4609\n",
      "Step 40: DPO Loss = 1.0391\n",
      "Step 50: DPO Loss = 0.6680\n",
      "Step 60: DPO Loss = 0.5078\n",
      "Step 70: DPO Loss = 1.1406\n",
      "Step 80: DPO Loss = 0.9297\n",
      "Step 90: DPO Loss = 0.4883\n",
      "Step 100: DPO Loss = 0.9688\n",
      "✅ 模型已保存至 /data/phybench/workdir/guosy/bw_workspace/outputs\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data/phybench/workdir/guosy/align/align-anything\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from align_anything.datasets.text_to_text.preference import PreferenceDataset\n",
    "from align_anything.configs.template import ChatTemplate\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 模型路径\n",
    "model_name = \"/data/phybench/workdir/guosy/align/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# 加载训练模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 加载参考模型（冻结参数）\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 初始化模板\n",
    "train_template = ChatTemplate(\n",
    "    formatter=tokenizer,\n",
    "    template=\"HOMEWORK\",\n",
    ")\n",
    "\n",
    "# 加载 DPO 偏好格式数据集\n",
    "dataset = PreferenceDataset(\n",
    "    path=\"/data/phybench/workdir/guosy/align/align_anything_t2t\",\n",
    "    template=train_template,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=tokenizer,\n",
    "    split=\"train\",\n",
    "    size=1000,\n",
    ")\n",
    "\n",
    "# DataLoader（DPO 每 batch 为 better + worse）\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=dataset.get_collator(),\n",
    "    sampler=RandomSampler(dataset),\n",
    "    batch_size=2,  # better + worse\n",
    ")\n",
    "\n",
    "# 优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 训练主循环\n",
    "model.train()\n",
    "scale_coeff = 0.1  # 等价于 beta\n",
    "\n",
    "for step, batch in enumerate(dataloader):\n",
    "    try:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        response_lens = batch[\"meta_info\"][\"response_lens\"]  # List[int]\n",
    "\n",
    "        half = input_ids.size(0) // 2\n",
    "        chosen_ids = input_ids[:half]\n",
    "        rejected_ids = input_ids[half:]\n",
    "        chosen_mask = attention_mask[:half]\n",
    "        rejected_mask = attention_mask[half:]\n",
    "        chosen_lens = response_lens[:half]\n",
    "        rejected_lens = response_lens[half:]\n",
    "\n",
    "        def get_logps(model, ids, mask, lens):\n",
    "            with torch.no_grad() if model is ref_model else torch.enable_grad():\n",
    "                outputs = model(input_ids=ids, attention_mask=mask)\n",
    "                logits = outputs.logits\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "                logp_list = []\n",
    "                for i, L in enumerate(lens):\n",
    "                    # 取 response 部分 token 段\n",
    "                    logp = log_probs[i, -L-1:-1, :]\n",
    "                    label = ids[i, -L:]\n",
    "                    logp = logp.gather(1, label.unsqueeze(-1)).squeeze(-1)\n",
    "                    logp_list.append(logp.sum())\n",
    "                return torch.stack(logp_list)\n",
    "\n",
    "        model_chosen_logp = get_logps(model, chosen_ids, chosen_mask, chosen_lens)\n",
    "        model_rejected_logp = get_logps(model, rejected_ids, rejected_mask, rejected_lens)\n",
    "\n",
    "        ref_chosen_logp = get_logps(ref_model, chosen_ids, chosen_mask, chosen_lens)\n",
    "        ref_rejected_logp = get_logps(ref_model, rejected_ids, rejected_mask, rejected_lens)\n",
    "\n",
    "        pi_diff = model_chosen_logp - model_rejected_logp\n",
    "        ref_diff = ref_chosen_logp - ref_rejected_logp\n",
    "        loss = -F.logsigmoid(scale_coeff * (pi_diff - ref_diff)).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}: DPO Loss = {loss.item():.4f}\")\n",
    "\n",
    "        if step == 100:\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[错误] 第 {step} 步处理失败：{e}\")\n",
    "        continue\n",
    "\n",
    "# 保存模型\n",
    "output_dir = \"/data/phybench/workdir/guosy/bw_workspace/outputs\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"✅ 模型已保存至 {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 测试DPO训练后的模型性能\n",
    "\n",
    "在训练结束后，我们试图测试训练后的模型对齐情况是否有所改观。\n",
    "\n",
    "### 5.1 加载新的模型权重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896, padding_idx=151643)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/data/phybench/workdir/guosy/bw_workspace/dpo_model\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# 将模型设置为eval模式\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 测试新模型的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text: If you encounter a wild animal in your local area that is aggressive towards humans, it is important to handle it with caution and respect. Here are some steps you can take:\n",
      "\n",
      "1. Do not approach or provoke the animal. This can be dangerous and may cause injury.\n",
      "2. Do not feed the animal. This can be harmful to both the animal and the human.\n",
      "3. Do not attempt to handle the animal. This can be dangerous and may cause injury.\n",
      "4. Do not attempt to remove the animal from the area. This can be dangerous and may cause injury.\n",
      "5. Do not attempt to capture or trap the animal. This can be dangerous and may cause injury.\n",
      "6. Do not attempt to harm the animal. This can be dangerous and may cause injury.\n",
      "7. Do not attempt to remove the animal from the area. This can be dangerous and may cause injury.\n",
      "8. Do not attempt to harm the animal. This can be dangerous and may cause injury.\n",
      "9. Do not attempt to capture or trap the animal. This can be dangerous and may cause injury.\n",
      "10. Do not attempt to remove the animal from the area. This can be dangerous and may cause injury.\n",
      "\n",
      "If you are unable to handle the animal, it is important to call emergency services. In some cases, the animal may be in danger and may need to be taken to a wildlife rehabilitation center. In other cases, the animal may be in danger and may need to be taken to a wildlife rehabilitation center. In any case, it is important to take the animal to a professional and not attempt to handle it yourself.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user queries.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Recently, a wild animal in the local area has become aggressive towards humans and caused several injuries. How should I handle this wild animal?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# the model generate new tokens\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=2048)\n",
    "# convert the generated tokens to text\n",
    "generated_text = tokenizer.decode(\n",
    "    output[0][len(inputs['input_ids'][0]) :], skip_special_tokens=True\n",
    ")\n",
    "print(\"\\nGenerated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 致谢\n",
    "\n",
    "- [Hugging Face Transformers 文档](https://huggingface.co/docs/transformers/index)\n",
    "- [DPO 论文](https://arxiv.org/abs/2305.18290)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
